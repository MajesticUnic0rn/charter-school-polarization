{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charter School NlP Fun Stuff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic processing\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "#networking and scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#text processing\n",
    "import spacy\n",
    "# import spacy_transformers #not really usable because of problems with cuda\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "# bring out the nuclear weapons for this job\n",
    "import os\n",
    "import openai\n",
    "#openai.api_key = os.getenv(\"openai\")\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download(\"puntk\")\n",
    "#nltk.data.path.append(\"C://Users//chris//AppData//Roaming//nltk_data\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import Levenshtein\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "input_data=pd.read_csv('data/active_charter_schools_report.csv')\n",
    "# The columns are just ugly, so let's clean them up\n",
    "new_columns = input_data.columns.str.replace(\"-\",\"\").str.replace(\" \",\"\").str.lower() \n",
    "input_data.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['website']=input_data['principal/directoremail'].str.split('@').str[-1]\n",
    "#list of columns I care about\n",
    "charter_school_column=['countydescription','schoolname','principal/directoremail','website']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[charter_school_column].head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search up board of directors of each google link and get ready to parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## search for the board of directors page from site\n",
    "\n",
    "def get_google_search_results_boards(school_name: str):\n",
    "    # set the school name to search for\n",
    "    # create a Google search query URL\n",
    "    query_url = f\"https://www.google.com/search?q={school_name}+meet+the+board+of+directors\"\n",
    "    # make a request to the query URL and get the HTML content\n",
    "    response = requests.get(query_url)\n",
    "    html_content = response.text\n",
    "    # parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # find all the search result links on the page\n",
    "    search_result_links = soup.find_all('a')\n",
    "    # extract the URLs of the first search result link from each search engine\n",
    "    result_urls = []\n",
    "    for link in search_result_links:\n",
    "        href = link.get('href')\n",
    "        if href.startswith('/url?q='):\n",
    "            result_url = href.split('/url?q=')[1].split('&')[0]\n",
    "            result_urls.append(result_url)\n",
    "        if len(result_urls) == 1:\n",
    "            break\n",
    "\n",
    "    # print the resulting URLs\n",
    "    return(result_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['board_of_directors_link']=input_data['schoolname'].apply(get_google_search_results_boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns=['countydescription','schoolname','principal/directoremail','zipcode5','boardchairfirstname','boardchairlastname','website','board_of_directors_link']\n",
    "input_data[feature_columns].to_csv('data/checkdata.csv',index=False)\n",
    "#checkpoint read input data\n",
    "#input_data = pd.read_csv('data/checkdata.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting feature google snippets are problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def get_google_feature_snippet(url):\n",
    "#     # Send a GET request to the URL and retrieve the HTML content\n",
    "#     response = requests.get(url)\n",
    "#     html = response.text\n",
    "    \n",
    "#     # Parse the HTML content using BeautifulSoup\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "#     # Find the Google feature snippet element, if one exists\n",
    "#     feature_snippet = soup.find('div', class_='kp-header')\n",
    "    \n",
    "#     # If a feature snippet was found, extract its text content and return it\n",
    "#     if feature_snippet:\n",
    "#         return feature_snippet.text.strip()\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add function that would determine if link is part of original domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def compare_domains(url1, url2):\n",
    "    domain1 = urlparse(url1).netloc.replace(\"www.\", \"\")\n",
    "    domain2 = urlparse(url2).netloc.replace(\"www.\", \"\")\n",
    "    return domain1 == domain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['link_domain_match']=input_data.apply(lambda x: compare_domains(f\"https://{x['website']}\",x['board_of_directors_link'][0]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[['link_domain_match','board_of_directors_link','website']].sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['link_domain_match'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data scrapper function for per string - some of these functions are used to test the data scrapper and whos the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each URL and scrape the text content\n",
    "def website_text_content(url: str):\n",
    "    # make a request to the URL and get the HTML content\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html_content = response.text\n",
    "    # parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # remove the header and footer from the HTML content\n",
    "    for tag in soup(['header', 'footer']):\n",
    "        tag.decompose()\n",
    "    # extract the text content from the HTML\n",
    "    text_content = soup.get_text().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').replace('.', ' ')\n",
    "\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = nltk.word_tokenize(text_content)\n",
    "    #filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words and len(token) > 1]\n",
    "    #filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    # remove extra spaces and line breaks\n",
    "    text_content = re.sub(r\"\\s+\", \" \", text_content).strip()\n",
    "    # add spaces between words that are capitalized\n",
    "    text_content = re.sub(r'(?<!^)(?=[A-Z])', ' ', text_content)\n",
    "    # remove punctuation\n",
    "    words = text_content.split()\n",
    "    # remove single word characters because of middle initials\n",
    "    filtered_words = [word for word in words if len(word) > 1]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "    # print the text content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_middle_content(url:str):\n",
    "    try:\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "        # Fetch the content from the URL\n",
    "        response = requests.get(url,headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while fetching the URL: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the main content container\n",
    "    main_container = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"div\", {\"role\": \"main\"})\n",
    "\n",
    "    if main_container is None:\n",
    "        print(\"Could not find the main content container.\")\n",
    "        return None\n",
    "\n",
    "    # Find the middle content\n",
    "    content_elements = main_container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'blockquote'])\n",
    "\n",
    "    if not content_elements:\n",
    "        print(\"Could not find any content elements.\")\n",
    "        return None\n",
    "\n",
    "    middle_index = len(content_elements) // 2\n",
    "    middle_content = content_elements[middle_index]\n",
    "\n",
    "    return middle_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_content(url):\n",
    "    try:\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "        response = requests.get(url,headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while fetching the URL: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Remove unwanted elements\n",
    "    for tag in ['header', 'footer', 'nav']:\n",
    "        elements_to_remove = soup.find_all(tag)\n",
    "        for elem in elements_to_remove:\n",
    "            elem.decompose()\n",
    "\n",
    "    # Extract text from the remaining elements\n",
    "    text_content = ' '.join(soup.stripped_strings)\n",
    "    # remove extra spaces and line breaks\n",
    "    text_content = re.sub(r\"\\s+\", \" \", text_content).strip()\n",
    "    # add spaces between words that are capitalized\n",
    "    text_content = re.sub(r'(?<!^)(?=[A-Z])', ' ', text_content)\n",
    "    # remove punctuation\n",
    "    words = text_content.split()\n",
    "    # remove single word characters because of middle initials\n",
    "    filtered_words = [word for word in words if len(word) > 1]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s: str):\n",
    "    # Remove digits\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    # Remove special characters\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    # Remove non-readable characters\n",
    "    s = ''.join(filter(lambda x: x.isprintable(), s))\n",
    "    return s\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation_and_single_chars(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Remove single character words\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if len(word) > 1]\n",
    "    result = ' '.join(filtered_words)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['board_of_directors_link']=input_data['board_of_directors_link'].astype(str)\n",
    "input_data['board_of_directors_link']=input_data['board_of_directors_link'].str.strip(\"[]\").str.strip(\"''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['director_text_content']=input_data['board_of_directors_link'].apply(extract_main_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data.to_csv('check_data_point2.csv',escapechar='\\\\', index=False)\n",
    "input_data = pd.read_csv('check_data_point2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine function to find multiple people - you should really try to experiment with different models because its really annoying to find people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "def extract_names_bert(text):\n",
    "    model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "    names = []\n",
    "    current_name_tokens = []\n",
    "    for token, label_id in zip(tokens, predictions[0]):\n",
    "        label = model.config.id2label[label_id.item()]\n",
    "        \n",
    "        if label == \"B-PER\":\n",
    "            if current_name_tokens:\n",
    "                names.append(tokenizer.convert_tokens_to_string(current_name_tokens))\n",
    "            current_name_tokens = [token]\n",
    "        elif label == \"I-PER\":\n",
    "            current_name_tokens.append(token)\n",
    "        else:\n",
    "            if current_name_tokens:\n",
    "                names.append(tokenizer.convert_tokens_to_string(current_name_tokens))\n",
    "                current_name_tokens = []\n",
    "\n",
    "    if current_name_tokens:\n",
    "        names.append(tokenizer.convert_tokens_to_string(current_name_tokens))\n",
    "\n",
    "    # Extract first and last names\n",
    "    \n",
    "    first_last_names = []\n",
    "    for name in names:\n",
    "        name_parts = name.split()\n",
    "        if len(name_parts) >= 2:\n",
    "            first_last_names.append((name_parts[0], name_parts[-1]))\n",
    "    \n",
    "    first_last_names=list(set(first_last_names))\n",
    "\n",
    "    return first_last_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Models are useless for NER stick to pretrain bert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names_sm(text_content: str):\n",
    "    nlp = spacy.load(\"en_core_web_sm\") # you can change the model to bigger ones like \"en_core_web_trf\" - problematic with cuda and spacy_transformers\n",
    "    # process the text with the NER model\n",
    "    doc = nlp(text_content)\n",
    "    # extract the named entities (people's names)\n",
    "    names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    # drop duplicates\n",
    "    names=list(set(names))\n",
    "    # print the names\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names_md(text_content: str):\n",
    "    nlp = spacy.load(\"en_core_web_md\") # you can change the model to bigger ones like \"en_core_web_trf\" - problematic with cuda and spacy_transformers\n",
    "    # process the text with the NER model\n",
    "    doc = nlp(text_content)\n",
    "    # extract the named entities (people's names)\n",
    "    names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    # Drop Duplicates\n",
    "    names=list(set(names))\n",
    "    # print the names\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names_lg(text_content: str):\n",
    "    nlp = spacy.load(\"en_core_web_lg\") # you can change the model to bigger ones like \"en_core_web_trf\" - problematic with cuda and spacy_transformers\n",
    "    # process the text with the NER model\n",
    "    doc = nlp(text_content)\n",
    "    # extract the named entities (people's names)\n",
    "    names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    # Drop Duplicates\n",
    "    names=list(set(names))\n",
    "    # print the names\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data['director_text_content'][40]\n",
    "(input_data['director_text_content'][90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['director_text_content'] = input_data['director_text_content'].astype(str)\n",
    "input_data['director_first_last']=input_data['director_text_content'].apply(extract_names_bert) # 10 minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## practically useless\n",
    "input_data['director_names_sm']=input_data['director_text_content'].apply(find_names_sm)\n",
    "input_data['director_names_md']=input_data['director_text_content'].apply(find_names_md)\n",
    "input_data['director_names_lg']=input_data['director_text_content'].apply(find_names_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charter_school_column=['lea','zipcode5','countydescription','schoolname','principal/directoremail','website','board_of_directors_link','link_domain_match','director_first_last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[charter_school_column].to_csv('check_data_point3.csv',escapechar='\\\\', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuclear option goooodbye monies\n",
    "\n",
    "10$ of my money is gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names_openai(text_content:str):\n",
    "  \n",
    "  response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=f\"context: {text_content} \\n\\n question: Who are the board members with first and last names?, give it to me as a python list If none are available give me an empty list.\",\n",
    "    temperature=0.86,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "  )\n",
    "  return(response.choices[0].text)\n",
    "\n",
    "# Define function to chunk a long text into smaller pieces\n",
    "\n",
    "def chunk_text(text, chunk_size=5000):\n",
    "    \"\"\"Chunk a string into a list of strings with a maximum size of chunk_size.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    while start < len(text):\n",
    "        if end >= len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size\n",
    "        end += chunk_size\n",
    "    return chunks\n",
    "\n",
    "def find_names_in_chunks(text_chunks:list):\n",
    "  \n",
    "  results = []\n",
    "  \n",
    "  for chunk in text_chunks:\n",
    "    names = find_names_openai(chunk)\n",
    "    results.append(names)\n",
    "  \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['clean_director_text_content']= input_data['director_text_content'].apply(clean_string)\n",
    "input_data['text_chunks'] = input_data['clean_director_text_content'].apply(chunk_text)\n",
    "#input_data['director_names_openai']=input_data['text_chunks'].apply(find_names_in_chunks) # that was 10$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data.to_csv('check_data_point4.csv',escapechar='\\\\', index=False)\n",
    "input_data = pd.read_csv('check_data_point4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checkpoint marker\n",
    "# charter_school_column=['countydescription','schoolname','principal/directoremail','website','director_names_openai']\n",
    "# input_data[charter_school_column].to_csv('director_name.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_cleaner(input_string:str):\n",
    "    input_string = input_string.replace('\\'','')\n",
    "    input_string = input_string.replace('[','')\n",
    "    input_string = input_string.replace(']','')\n",
    "    input_string = input_string.replace('\\\"','')\n",
    "    input_string = input_string.replace(\"NA\", \"\")\n",
    "    input_string = input_string.replace(\"\\n\", \"\")\n",
    "    input_string = input_string.replace(\"\\\\\\\\n\\\\\\\\n\", \"\")\n",
    "    input_string=input_string.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    input_string=input_string.replace(\"\\\\\\\\n\", \"\")\n",
    "    input_string=input_string.replace(\"Answer:\", \"\")\n",
    "    input_string = re.sub(r'(\\n[\\s]*)+', ' ', input_string)\n",
    "    # Remove patterns such as \\n\\nAnswer:, \\n\\n(, ), and (list)\n",
    "    patterns = ['Answer:', '\\(\\w+\\)', '\\(list\\)']\n",
    "    for pattern in patterns:\n",
    "        input_string = re.sub(r'\\n\\n{}[\\s]*'.format(pattern), ' ', input_string)\n",
    "    # Remove leading and trailing whitespaces\n",
    "    input_string = input_string.strip()\n",
    "    words_to_remove = ['Director', 'Assistant', 'Parent Seat','Parent', 'Seat', 'Community', 'Board', 'Treasurer','Alumni','-','Business Analyst']\n",
    "    \n",
    "    for word in words_to_remove:\n",
    "        input_string = input_string.replace(word, \"\")\n",
    "    \n",
    "    return input_string \n",
    "\n",
    "# def second_cleaner(input_string:list):\n",
    "#     words_to_remove = ['Director', 'Assistant', 'Parent Seat','Parent', 'Seat', 'Community', 'Board', 'Treasurer','Alumni','-']\n",
    "#     for word in words_to_remove:\n",
    "#         input_string = input_string.str.replace(word, '', regex=False)\n",
    "#     return input_string\n",
    "\n",
    "# def remove_words(words_list: list):\n",
    "#     remove_list = ['Director', 'Assistant', 'Parent Seat','Parent', 'Seat', 'Community', 'Board', 'Treasurer','Alumni','-']\n",
    "#     return [word for word in words_list if word not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['director_names_openai_clean']=input_data['director_names_openai'].apply(openai_cleaner)\n",
    "input_data['cleaner_names'] = input_data['director_names_openai_clean'].apply(find_names_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['cleaner_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['cleaner_names'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(input_data[input_data['cleaner_names'] == '[]' ])\n",
    "def count_empty_lists(lst):\n",
    "    return len(lst) == 0\n",
    "# apply the function to each element of the column and count the number of empty lists\n",
    "num_empty_lists = input_data['cleaner_names'].apply(count_empty_lists).sum()\n",
    "print(num_empty_lists) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 74 schools that dont have a list of board of directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[['schoolname','officialschoolname','zipcode5','Hyperlink','cleaner_names']].to_csv('names_cleaning.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model has a problem with black names :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Sarah Crofer and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "names = []\n",
    "this_name = []\n",
    "all_names_list_tmp = []\n",
    "\n",
    "for ner_dict in ner_results:\n",
    "    if ner_dict['entity'] == 'B-PER':\n",
    "        if len(this_name) == 0:\n",
    "            this_name.append(ner_dict['word'])\n",
    "        else:\n",
    "            all_names_list_tmp.append([this_name])\n",
    "            this_name = []\n",
    "            this_name.append(ner_dict['word'])\n",
    "    elif ner_dict['entity'] == 'I-PER':\n",
    "        this_name.append(ner_dict['word'])\n",
    "\n",
    "all_names_list_tmp.append([this_name])\n",
    "\n",
    "print(all_names_list_tmp)\n",
    "\n",
    "final_name_list = []\n",
    "for name_list in all_names_list_tmp:\n",
    "    full_name = ' '.join(name_list[0]).replace(' ##', '').replace(' .', '.')\n",
    "    final_name_list.append([full_name])\n",
    "\n",
    "print(final_name_list)\n",
    "\n",
    "# for result in ner_results:\n",
    "#     if result['entity'] == 'B-PER' or result['entity'] == 'I-PER':\n",
    "#         names.append(result['word'])\n",
    "\n",
    "#print(names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding archive, we probably could use this to make it better next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "# Load the spaCy model for named entity recognition\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define the text to analyze\n",
    "text = test_content\n",
    "\n",
    "# Tokenize the text and convert to PyTorch tensors\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_tensor = torch.tensor([token_ids])\n",
    "\n",
    "# Run the text through the BERT model\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(token_tensor)[0]\n",
    "\n",
    "# Extract the named entities using spaCy\n",
    "named_entities = []\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        named_entities.append(ent.text)\n",
    "\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holy shit finding peoples names are difficult\n",
    "## refine people's names and seperate by first and last name - find likely voter registration affilation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voter Lookup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter=pd.read_csv('data/ncvoter_Statewide.txt',sep=\"\\t\", header=0,encoding='ISO-8859-1')\n",
    "#input_data_copy = pd.read_excel('data/check_data_point3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter[['voter_reg_num','voter_status_desc','first_name','last_name','birth_year','zip_code','race_code','ethnic_code','party_cd','gender_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter=state_voter.loc[(state_voter['voter_status_desc'] == 'ACTIVE') & (state_voter['zip_code'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter[(state_voter['first_name']=='CARY') & (state_voter['last_name']=='CAIN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter=state_voter.dropna(subset=['first_name','last_name'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter[['first_name','last_name']]=state_voter[['first_name','last_name']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter['full_name'] = state_voter[['first_name','last_name']].apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zipcode calculator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance lexical name similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_similarity(name, name_column):\n",
    "    name_column = name_column.apply(lambda x: Levenshtein.ratio(name, x))\n",
    "    return name_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_number = 3\n",
    "print(input_data['schoolname'][index_number])\n",
    "print(input_data['zipcode5'][index_number])\n",
    "input_data_copy['cleaner_names'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/US.txt\"\n",
    "us_zip = pd.read_table(file_path, header=None)\n",
    "us_zip.columns = [\"country_code\",\"postal_code\", \"place_name\", \"admin_name1\", \"admin_code1\",\n",
    "              \"admin_name2\", \"admin_code2\", \"admin_name3\", \"admin_code3\",\n",
    "              \"latitude\", \"longitude\", \"accuracy\"]\n",
    "us_zip['postal_code'] = us_zip['postal_code'].apply(lambda x : str(x).zfill(5))\n",
    "state_voter['zip_code']=state_voter['zip_code'].astype(int)\n",
    "state_voter['zip_code']=state_voter['zip_code'].astype(str)\n",
    "state_voter=pd.merge(state_voter,us_zip[['postal_code', 'latitude', 'longitude']],left_on='zip_code',right_on='postal_code',how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_to_compare = 'BRANDON RUSSELL'\n",
    "# similarity_scores = lexical_similarity(name_to_compare, state_voter['full_name'])\n",
    "# state_voter['Similarity'] = similarity_scores\n",
    "# parse_zip=state_voter[['full_name','party_cd','Similarity','zip_code','latitude','longitude']].sort_values(by='Similarity', ascending=False).head(50)\n",
    "# parse_zip.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def calculate_distance(lat, lon, lat_column, lon_column):\n",
    "    distance_column = []\n",
    "    for lat2, lon2 in zip(lat_column, lon_column):\n",
    "        distance = geodesic((lat, lon), (lat2, lon2)).miles\n",
    "        distance_column.append(distance)\n",
    "    return pd.Series(distance_column)\n",
    "\n",
    "# # Reference coordinates (latitude, longitude)\n",
    "# zipcode = '27217'\n",
    "# ref_lat = us_zip['latitude'].loc[us_zip['postal_code'] == zipcode].iloc[0]\n",
    "# ref_lon = us_zip['longitude'].loc[us_zip['postal_code'] == zipcode].iloc[0]\n",
    "\n",
    "# # Calculate distances and add as a new column in the DataFrame\n",
    "# parse_zip['Distance']=calculate_distance(ref_lat, ref_lon, parse_zip['latitude'], parse_zip['longitude'])\n",
    "\n",
    "#parse_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board_party_affilation=parse_zip.sort_values(by = ['Similarity', 'Distance'], ascending = [False,True]).head(1)['party_cd'].values.tolist()\n",
    "# party_list =[]\n",
    "# party_list.append(board_party_affilation[0])\n",
    "# party_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take note of this issue\n",
    "state_voter[state_voter[\"latitude\"].isnull() | state_voter[\"longitude\"].isnull()]['zip_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_zip[us_zip.postal_code == '27838']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_political_affilation(list_names:list,lat,lon):\n",
    "    party_list =[]\n",
    "    for names in list_names:\n",
    "        name_to_compare = names.upper()\n",
    "        #print(name_to_compare)\n",
    "        similarity_scores = lexical_similarity(name_to_compare, state_voter['full_name'])\n",
    "        state_voter['Similarity'] = similarity_scores\n",
    "        parse_zip=state_voter[['full_name','party_cd','Similarity','zip_code','latitude','longitude']].sort_values(by='Similarity', ascending=False).head(50)\n",
    "        parse_zip.reset_index(inplace=True)\n",
    "        ref_lat = lat\n",
    "        ref_lon = lon\n",
    "        parse_zip['Distance']=calculate_distance(ref_lat, ref_lon, parse_zip['latitude'], parse_zip['longitude'])\n",
    "        board_party_affilation=parse_zip.sort_values(by = ['Similarity', 'Distance'], ascending = [False,True]).head(1)['party_cd'].values.tolist()\n",
    "        party_list.append(board_party_affilation[0])\n",
    "    return party_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from typing import List, Tuple\n",
    "#gpt4 political affiliation help on how to do this\n",
    "def optimize_find_political_affiliation(list_names: List[str], lat: float, lon: float) -> List[str]:\n",
    "    party_list = []\n",
    "    for name in list_names:\n",
    "        name = name.upper()\n",
    "        scores = []\n",
    "        for i, voter_name in enumerate(state_voter['full_name']):\n",
    "            similarity = lexical_similarity(name, voter_name)\n",
    "            if len(scores) < 50:\n",
    "                # If we don't have 50 scores yet, just add it to the heap.\n",
    "                heapq.heappush(scores, (similarity, i))\n",
    "            else:\n",
    "                # If we already have 50 scores, add the new one and remove the smallest.\n",
    "                heapq.heappushpop(scores, (similarity, i))\n",
    "        # Get the indices of the top 50 most similar names.\n",
    "        top_indices = [i for _, i in scores]\n",
    "        # Calculate distances for these 50 records.\n",
    "        distances = calculate_distance(lat, lon, state_voter.loc[top_indices, 'latitude'], state_voter.loc[top_indices, 'longitude'])\n",
    "        # Pair up similarities and distances, then sort by similarity first and distance second.\n",
    "        sorted_records = sorted(zip(scores, distances), key=lambda x: (-x[0][0], x[1]))\n",
    "        # Get the political affiliation of the top record.\n",
    "        top_record_index = sorted_records[0][0][1]\n",
    "        top_party = state_voter.loc[top_record_index, 'party_cd']\n",
    "        party_list.append(top_party)\n",
    "    return party_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_copy=pd.read_csv('data/final_checkpoint.csv',encoding='latin-1')\n",
    "## data preprocessing after reading\n",
    "input_data_copy['zipcode']=input_data_copy['zipcode'].astype(str)\n",
    "input_data_copy['cleaner_names']=input_data_copy['cleaner_names'].apply(ast.literal_eval)\n",
    "#input_data_copy=pd.merge(input_data_copy,us_zip[['postal_code', 'latitude', 'longitude']],left_on='zipcode',right_on='postal_code',how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_copy['political_affilation']=input_data_copy.apply(lambda x: find_political_affilation(x['cleaner_names'],x['latitude'],x['longitude']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_copy.to_csv('data/final_checkpoint_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_political_affilation(input_data_copy_test['cleaner_names'].apply(ast.literal_eval)[0],input_data_copy_test['zipcode'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dem(lst):\n",
    "    return lst.count('DEM')\n",
    "def count_rep(lst):\n",
    "    return lst.count('REP')\n",
    "def count_una(lst):\n",
    "    return lst.count('UNA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_copy['count_dem']=input_data_copy['political_affilation'].apply(count_dem)\n",
    "input_data_copy['count_rep']=input_data_copy['political_affilation'].apply(count_rep)\n",
    "input_data_copy['count_una']=input_data_copy['political_affilation'].apply(count_una)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_copy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_copy.sort_values(by = ['count_dem'], ascending = [False]).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_copy.to_csv('data/final_checkpoint_clean.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8f64c448f76f946c764b774ed60aa887bf5a3d0ad1d319c91e87b0f7b7d7e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
