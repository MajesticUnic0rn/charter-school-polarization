{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        with requests.get(url, headers=headers) as response:\n",
    "            response.raise_for_status()\n",
    "            try:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                for tag in ['head','header', 'footer','foot,''navigation','nav','dropdown']:\n",
    "                    elements_to_remove = soup.find_all(tag)\n",
    "                    for elem in elements_to_remove:\n",
    "                        elem.decompose()\n",
    "                # Extract text from the remaining elements\n",
    "                text_content = ' '.join(soup.stripped_strings)\n",
    "                # remove extra spaces and line breaks\n",
    "                text_content = re.sub(r\"\\s+\", \" \", text_content).strip()\n",
    "                # add spaces between words that are capitalized\n",
    "                text_content = re.sub(r'(?<!^)(?=[A-Z])', ' ', text_content)\n",
    "                # remove punctuation\n",
    "                words = text_content.split()\n",
    "                # remove single word characters because of middle initials\n",
    "                filtered_words = [word for word in words if len(word) > 1]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                return filtered_text\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while parsing HTML for URL {url}: {e}\")\n",
    "                return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while processing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_links(links):\n",
    "    max_threads = 10\n",
    "    with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "        futures_to_link = {executor.submit(extract_content, link): link for link in links}\n",
    "        for future in as_completed(futures_to_link):\n",
    "            link = futures_to_link[future]\n",
    "            try:\n",
    "                extracted_content = future.result()\n",
    "                # Process the extracted content as needed\n",
    "                # ...\n",
    "                return extracted_content\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing URL {link}: {e}\")\n",
    "\n",
    "# Assuming you have a list of links stored in the 'links' variable\n",
    "links = ['https://www.charlottelabschool.org/']  # Your list of links\n",
    "process_links(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class CharterSpider(scrapy.Spider):\n",
    "    name = \"charter_spider\"\n",
    "\n",
    "    start_urls = [\n",
    "        \"https://www.charlottesecondary.org/\",  # Replace with the actual charter school website URL\n",
    "    ]\n",
    "\n",
    "    custom_settings = {\n",
    "        'DEPTH_LIMIT': 1,  # Limit the depth of the scraping to only one level\n",
    "    }\n",
    "\n",
    "    link_counter = 0  # Initialize the link counter\n",
    "    link_limit = 200  # Set the desired link limit\n",
    "    keywords = ['calendar','events','schedule']  # Add more keywords as needed\n",
    "    # Extract root domain to restrict link extraction\n",
    "    parsed_uri = urlparse(start_urls[0])\n",
    "    root_domain = parsed_uri.netloc\n",
    "\n",
    "    def parse(self, response):\n",
    "        link_extractor = LinkExtractor(allow_domains=self.root_domain)\n",
    "        links = link_extractor.extract_links(response)\n",
    "\n",
    "        for link in links:\n",
    "            url = link.url\n",
    "            # Process the link further if it doesn't match the calendar keywords\n",
    "            if any(keyword in url.lower() for keyword in self.keywords):\n",
    "                continue\n",
    "\n",
    "            if self.link_counter >= self.link_limit:\n",
    "                break  # Break the loop if the link limit is reached\n",
    "            \n",
    "            self.link_counter += 1\n",
    "            yield scrapy.Request(url, callback=self.parse, follow=True)\n",
    "            \n",
    "            yield {\n",
    "                \"url\": url\n",
    "            }\n",
    "            self.link_counter += 1  # Increment the link counter\n",
    "            print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "process = CrawlerProcess({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'LOG_LEVEL': 'INFO',\n",
    "    'DEPTH_LIMIT': 1\n",
    "})\n",
    "# Start the spider\n",
    "process.crawl(CharterSpider)\n",
    "process.start() # the script will block here until the crawling is finished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "class CollectingPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.data = []\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        # Do something with the collected items here\n",
    "        print(self.data)\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.data.append(item)\n",
    "        return item\n",
    "\n",
    "settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': 'INFO',\n",
    "    'DEPTH_LIMIT': 3,\n",
    "    'ITEM_PIPELINES': {'__main__.CollectingPipeline': 1},  # Used for pipeline 1\n",
    "}\n",
    "\n",
    "configure_logging()\n",
    "runner = CrawlerRunner(settings)\n",
    "d = runner.crawl(CharterSpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd read back merge_charter for school name\n",
    "#copy and paste google search link for each school name \n",
    "#use school homepage for webcrawler\n",
    "#explode webcrawler results \n",
    "#scrape information from each school website \n",
    "#create embeddings for each school website using openai \n",
    "#Faiss index search up for school website embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "input_data=pd.read_csv(\"processed_data/merge_charter_district.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## process the the home page url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## search for the board of directors page from site\n",
    "\n",
    "def get_google_search_school_website(school_name: str):\n",
    "    # set the school name to search for\n",
    "    # create a Google search query URL\n",
    "    query_url = f\"https://www.google.com/search?q={school_name}+charter+school+north+carolina\"\n",
    "    # make a request to the query URL and get the HTML content\n",
    "    response = requests.get(query_url)\n",
    "    html_content = response.text\n",
    "    # parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # find all the search result links on the page\n",
    "    search_result_links = soup.find_all('a')\n",
    "    # extract the URLs of the first search result link from each search engine\n",
    "    result_urls = []\n",
    "    for link in search_result_links:\n",
    "        href = link.get('href')\n",
    "        if href.startswith('/url?q='):\n",
    "            result_url = href.split('/url?q=')[1].split('&')[0]\n",
    "            result_urls.append(result_url)\n",
    "        if len(result_urls) == 1:\n",
    "            break\n",
    "\n",
    "    # print the resulting URLs\n",
    "    return(result_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['school_homepage']=input_data['schoolname'].apply(get_google_search_school_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['school_homepage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['school_homepage']=input_data['school_homepage'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['school_homepage'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.to_csv(\"processed_data/merge_charter_district.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(input_data['school_homepage'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['school_homepage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_crawler=pd.read_csv(\"crawler/crawler_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.communitydva.org/apps/pages/index....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.communitydva.org/apps/video/watch....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.durhamcharter.org/privacy-policy/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.joycharter.org/virtual-tour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.joycharter.org/troubleshooting-videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>https://www.wilsonpreparatoryacademy.org/apps/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>https://www.wilsonpreparatoryacademy.org/apps/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>https://www.wilsonpreparatoryacademy.org/apps/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>https://www.wilsonpreparatoryacademy.org/apps/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>https://www.wilsonpreparatoryacademy.org/apps/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4896 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url\n",
       "0     https://www.communitydva.org/apps/pages/index....\n",
       "1     https://www.communitydva.org/apps/video/watch....\n",
       "2         https://www.durhamcharter.org/privacy-policy/\n",
       "3               https://www.joycharter.org/virtual-tour\n",
       "4     https://www.joycharter.org/troubleshooting-videos\n",
       "...                                                 ...\n",
       "4891  https://www.wilsonpreparatoryacademy.org/apps/...\n",
       "4892  https://www.wilsonpreparatoryacademy.org/apps/...\n",
       "4893  https://www.wilsonpreparatoryacademy.org/apps/...\n",
       "4894  https://www.wilsonpreparatoryacademy.org/apps/...\n",
       "4895  https://www.wilsonpreparatoryacademy.org/apps/...\n",
       "\n",
       "[4896 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_crawler['root_domain']=web_crawler['url'].apply(lambda url: urlparse(url).netloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        www.myncca.com\n",
       "1                          ncva.k12.com\n",
       "2                    www.joycharter.org\n",
       "3                 www.durhamcharter.org\n",
       "4                  www.communitydva.org\n",
       "                     ...               \n",
       "201    www.tworiverscommunityschool.net\n",
       "202              www.dillardacademy.org\n",
       "203                           wpanc.net\n",
       "204         www.salliebhowardschool.com\n",
       "205    www.wilsonpreparatoryacademy.org\n",
       "Name: school_homepage, Length: 206, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.school_homepage.apply(lambda url: urlparse(url).netloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(url):\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc\n",
    "    if domain.startswith(\"www.\"):\n",
    "        domain = domain[4:]\n",
    "    return domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['root_domain']=input_data['school_homepage'].apply(normalize)\n",
    "web_crawler['root_domain']=web_crawler['url'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data=pd.merge(input_data, web_crawler, on='root_domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['countydescription', 'political_afil', 'extract_name_clean_list',\n",
       "       'schoolname', 'principal/directoremail', 'website',\n",
       "       'board_of_directors_link', 'link_domain_match', 'director_first_last',\n",
       "       'cleaner_names', 'zipcode', 'political_affilation', 'postal_code',\n",
       "       'latitude', 'longitude', 'count_dem', 'count_rep', 'count_una',\n",
       "       'district_count_dem', 'district_count_rep', 'district_count_una',\n",
       "       'school_homepage', 'root_domain', 'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[['schoolname','url']].to_csv('processed_data/charter_links.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
