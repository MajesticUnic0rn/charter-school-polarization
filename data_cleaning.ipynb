{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charter School NlP Fun Stuff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic processing\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "#networking and scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#text processing\n",
    "import spacy\n",
    "# import spacy_transformers #not really usable because of problems with cuda\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "# bring out the nuclear weapons for this job\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"openai\")\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download(\"puntk\")\n",
    "#nltk.data.path.append(\"C://Users//chris//AppData//Roaming//nltk_data\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "input_data=pd.read_csv('data/active_charter_schools_report.csv')\n",
    "# The columns are just ugly, so let's clean them up\n",
    "new_columns = input_data.columns.str.replace(\"-\",\"\").str.replace(\" \",\"\").str.lower() \n",
    "input_data.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['website']=input_data['principal/directoremail'].str.split('@').str[-1]\n",
    "#list of columns I care about\n",
    "charter_school_column=['countydescription','schoolname','principal/directoremail','website']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[charter_school_column].head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search up board of directors of each google link and get ready to parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_google_search_results_boards(school_name: str):\n",
    "    # set the school name to search for\n",
    "    # create a Google search query URL\n",
    "    query_url = f\"https://www.google.com/search?q={school_name}+meet+the+board+of+directors\"\n",
    "    # make a request to the query URL and get the HTML content\n",
    "    response = requests.get(query_url)\n",
    "    html_content = response.text\n",
    "    # parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # find all the search result links on the page\n",
    "    search_result_links = soup.find_all('a')\n",
    "    # extract the URLs of the first search result link from each search engine\n",
    "    result_urls = []\n",
    "    for link in search_result_links:\n",
    "        href = link.get('href')\n",
    "        if href.startswith('/url?q='):\n",
    "            result_url = href.split('/url?q=')[1].split('&')[0]\n",
    "            result_urls.append(result_url)\n",
    "        if len(result_urls) == 1:\n",
    "            break\n",
    "\n",
    "    # print the resulting URLs\n",
    "    return(result_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['board_of_directors_link']=input_data['schoolname'].apply(get_google_search_results_boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data.to_csv('checkdata.csv',index=False)\n",
    "#checkpoint read input data\n",
    "input_data = pd.read_csv('checkdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each URL and scrape the text content\n",
    "def website_text_content(url: str):\n",
    "    # make a request to the URL and get the HTML content\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html_content = response.text\n",
    "    # parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # remove the header and footer from the HTML content\n",
    "    for tag in soup(['header', 'footer']):\n",
    "        tag.decompose()\n",
    "    # extract the text content from the HTML\n",
    "    text_content = soup.get_text().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').replace('.', ' ')\n",
    "\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = nltk.word_tokenize(text_content)\n",
    "    #filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words and len(token) > 1]\n",
    "    #filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    # remove extra spaces and line breaks\n",
    "    text_content = re.sub(r\"\\s+\", \" \", text_content).strip()\n",
    "    # add spaces between words that are capitalized\n",
    "    text_content = re.sub(r'(?<!^)(?=[A-Z])', ' ', text_content)\n",
    "    # remove punctuation\n",
    "    words = text_content.split()\n",
    "    # remove single word characters because of middle initials\n",
    "    filtered_words = [word for word in words if len(word) > 1]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "    # print the text content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s: str):\n",
    "    # Remove digits\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    # Remove special characters\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    # Remove non-readable characters\n",
    "    s = ''.join(filter(lambda x: x.isprintable(), s))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['Hyperlink'] = input_data['board_of_directors'].str.strip(\"[]\").str.strip(\"''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['director_text_content']=input_data['Hyperlink'].apply(website_text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data.to_csv('check_data_point2.csv',escapechar='\\\\', index=False)\n",
    "#test_input = pd.read_csv('check_data_point2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine function to find multiple people - you should really try to experiment with different models because its really annoying to find people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names_sm(text_content: str):\n",
    "    nlp = spacy.load(\"en_core_web_sm\") # you can change the model to bigger ones like \"en_core_web_trf\" - problematic with cuda and spacy_transformers\n",
    "    # process the text with the NER model\n",
    "    doc = nlp(text_content)\n",
    "    # extract the named entities (people's names)\n",
    "    names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    # drop duplicates\n",
    "    names=list(set(names))\n",
    "    # print the names\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names_md(text_content: str):\n",
    "    nlp = spacy.load(\"en_core_web_md\") # you can change the model to bigger ones like \"en_core_web_trf\" - problematic with cuda and spacy_transformers\n",
    "    # process the text with the NER model\n",
    "    doc = nlp(text_content)\n",
    "    # extract the named entities (people's names)\n",
    "    names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    # Drop Duplicates\n",
    "    names=list(set(names))\n",
    "    # print the names\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names_lg(text_content: str):\n",
    "    nlp = spacy.load(\"en_core_web_lg\") # you can change the model to bigger ones like \"en_core_web_trf\" - problematic with cuda and spacy_transformers\n",
    "    # process the text with the NER model\n",
    "    doc = nlp(text_content)\n",
    "    # extract the named entities (people's names)\n",
    "    names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    # Drop Duplicates\n",
    "    names=list(set(names))\n",
    "    # print the names\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['director_names_sm']=input_data['director_text_content'].apply(find_names_sm)\n",
    "input_data['director_names_md']=input_data['director_text_content'].apply(find_names_md)\n",
    "input_data['director_names_lg']=input_data['director_text_content'].apply(find_names_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data.to_csv('check_data_point3.csv',escapechar='\\\\', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuclear option goooodbye monies\n",
    "\n",
    "10$ of my money is gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names_openai(text_content:str):\n",
    "  \n",
    "  response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=f\"context: {text_content} \\n\\n question: Who are the board members with first and last names?, give it to me as a python list If none are available give me NA\",\n",
    "    temperature=0.86,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "  )\n",
    "  return(response.choices[0].text)\n",
    "\n",
    "# Define function to chunk a long text into smaller pieces\n",
    "\n",
    "def chunk_text(text, chunk_size=3000):\n",
    "    \"\"\"Chunk a string into a list of strings with a maximum size of chunk_size.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    while start < len(text):\n",
    "        if end >= len(text):\n",
    "            end = len(text)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size\n",
    "        end += chunk_size\n",
    "    return chunks\n",
    "\n",
    "def find_names_in_chunks(text_chunks:list):\n",
    "  \n",
    "  results = []\n",
    "  \n",
    "  for chunk in text_chunks:\n",
    "    names = find_names_openai(chunk)\n",
    "    results.append(names)\n",
    "  \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['clean_director_text_content']= input_data['director_text_content'].apply(clean_string)\n",
    "input_data['text_chunks'] = input_data['clean_director_text_content'].apply(chunk_text)\n",
    "#input_data['director_names_openai']=input_data['text_chunks'].apply(find_names_in_chunks) # that was 10$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data.to_csv('check_data_point4.csv',escapechar='\\\\', index=False)\n",
    "input_data = pd.read_csv('check_data_point4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checkpoint marker\n",
    "# charter_school_column=['countydescription','schoolname','principal/directoremail','website','director_names_openai']\n",
    "# input_data[charter_school_column].to_csv('director_name.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_cleaner(input_string:str):\n",
    "    input_string = input_string.replace('\\'','')\n",
    "    input_string = input_string.replace('[','')\n",
    "    input_string = input_string.replace(']','')\n",
    "    input_string = input_string.replace('\\\"','')\n",
    "    input_string = input_string.replace(\"NA\", \"\")\n",
    "    input_string = input_string.replace(\"\\n\", \"\")\n",
    "    input_string = input_string.replace(\"\\\\\\\\n\\\\\\\\n\", \"\")\n",
    "    input_string=input_string.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    input_string=input_string.replace(\"\\\\\\\\n\", \"\")\n",
    "    input_string=input_string.replace(\"Answer:\", \"\")\n",
    "    input_string = re.sub(r'(\\n[\\s]*)+', ' ', input_string)\n",
    "    # Remove patterns such as \\n\\nAnswer:, \\n\\n(, ), and (list)\n",
    "    patterns = ['Answer:', '\\(\\w+\\)', '\\(list\\)']\n",
    "    for pattern in patterns:\n",
    "        input_string = re.sub(r'\\n\\n{}[\\s]*'.format(pattern), ' ', input_string)\n",
    "    # Remove leading and trailing whitespaces\n",
    "    input_string = input_string.strip()\n",
    "    words_to_remove = ['Director', 'Assistant', 'Parent Seat','Parent', 'Seat', 'Community', 'Board', 'Treasurer','Alumni','-','Business Analyst']\n",
    "    \n",
    "    for word in words_to_remove:\n",
    "        input_string = input_string.replace(word, \"\")\n",
    "    \n",
    "    return input_string \n",
    "\n",
    "# def second_cleaner(input_string:list):\n",
    "#     words_to_remove = ['Director', 'Assistant', 'Parent Seat','Parent', 'Seat', 'Community', 'Board', 'Treasurer','Alumni','-']\n",
    "#     for word in words_to_remove:\n",
    "#         input_string = input_string.str.replace(word, '', regex=False)\n",
    "#     return input_string\n",
    "\n",
    "# def remove_words(words_list: list):\n",
    "#     remove_list = ['Director', 'Assistant', 'Parent Seat','Parent', 'Seat', 'Community', 'Board', 'Treasurer','Alumni','-']\n",
    "#     return [word for word in words_list if word not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['director_names_openai_clean']=input_data['director_names_openai'].apply(openai_cleaner)\n",
    "input_data['cleaner_names'] = input_data['director_names_openai_clean'].apply(find_names_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['cleaner_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['cleaner_names'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(input_data[input_data['cleaner_names'] == '[]' ])\n",
    "def count_empty_lists(lst):\n",
    "    return len(lst) == 0\n",
    "# apply the function to each element of the column and count the number of empty lists\n",
    "num_empty_lists = input_data['cleaner_names'].apply(count_empty_lists).sum()\n",
    "print(num_empty_lists) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 74 schools that dont have a list of board of directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[['schoolname','officialschoolname','zipcode5','Hyperlink','cleaner_names']].to_csv('names_cleaning.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model has a problem with black names :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Sarah Crofer and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "names = []\n",
    "this_name = []\n",
    "all_names_list_tmp = []\n",
    "\n",
    "for ner_dict in ner_results:\n",
    "    if ner_dict['entity'] == 'B-PER':\n",
    "        if len(this_name) == 0:\n",
    "            this_name.append(ner_dict['word'])\n",
    "        else:\n",
    "            all_names_list_tmp.append([this_name])\n",
    "            this_name = []\n",
    "            this_name.append(ner_dict['word'])\n",
    "    elif ner_dict['entity'] == 'I-PER':\n",
    "        this_name.append(ner_dict['word'])\n",
    "\n",
    "all_names_list_tmp.append([this_name])\n",
    "\n",
    "print(all_names_list_tmp)\n",
    "\n",
    "final_name_list = []\n",
    "for name_list in all_names_list_tmp:\n",
    "    full_name = ' '.join(name_list[0]).replace(' ##', '').replace(' .', '.')\n",
    "    final_name_list.append([full_name])\n",
    "\n",
    "print(final_name_list)\n",
    "\n",
    "# for result in ner_results:\n",
    "#     if result['entity'] == 'B-PER' or result['entity'] == 'I-PER':\n",
    "#         names.append(result['word'])\n",
    "\n",
    "#print(names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding archive, we probably could use this to make it better next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "# Load the spaCy model for named entity recognition\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define the text to analyze\n",
    "text = test_content\n",
    "\n",
    "# Tokenize the text and convert to PyTorch tensors\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_tensor = torch.tensor([token_ids])\n",
    "\n",
    "# Run the text through the BERT model\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(token_tensor)[0]\n",
    "\n",
    "# Extract the named entities using spaCy\n",
    "named_entities = []\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        named_entities.append(ent.text)\n",
    "\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text_content)\n",
    "\n",
    "# Initialize empty lists to store first and last names\n",
    "first_names = []\n",
    "last_names = []\n",
    "\n",
    "# Iterate through the entities in the document and check if they are a person entity\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        # Split the entity text into tokens\n",
    "        tokens = ent.text.split()\n",
    "        \n",
    "        # If there are two or more tokens, assume the first is the first name and the last is the last name\n",
    "        if len(tokens) >= 2:\n",
    "            first_names.append(tokens[0])\n",
    "            last_names.append(tokens[-1])\n",
    "            \n",
    "# Print the extracted first and last names\n",
    "print(\"First names:\", first_names)\n",
    "print(\"Last names:\", last_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holy shit finding peoples names are difficult\n",
    "\n",
    "## refine people's names and seperate by first and last name - find likely voter registration affilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voter Lookup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter=pd.read_csv('ncvoter_Statewide.txt',sep=\"\\t\", header=0,encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter[['voter_reg_num','first_name','last_name','birth_year','zip_code','race_code','ethnic_code','party_cd','gender_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_voter[(state_voter['first_name']=='CARY') & (state_voter['last_name']=='CAIN')].to_csv('voter_example.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8f64c448f76f946c764b774ed60aa887bf5a3d0ad1d319c91e87b0f7b7d7e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
